<!DOCTYPE html>
<html class="no-js" lang="zh-CN">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>机器学习学习笔记--线性回归</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta name="generator" content="Hugo 0.53" />
	<meta property="og:title" content="机器学习学习笔记--线性回归" />
<meta property="og:description" content="Andrew NG&#39;s Machine Learning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zcheng.ren/machinelearning/machinelearninglinearregression/" /><meta property="article:published_time" content="2019-01-19T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2019-01-19T00:00:00&#43;00:00"/>

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习学习笔记--线性回归"/>
<meta name="twitter:description" content="Andrew NG&#39;s Machine Learning."/>

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		

<script>
	hljs.initHighlightingOnLoad();
</script>

<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Zeech&#39;s Tech Blog" rel="home">
				<div class="logo__title">Zeech&#39;s Tech Blog</div>
				<div class="logo__tagline">Vimer|后端开发|博客er|机器学习freshman</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">博客首页</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/redis/">Redis</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/machinelearning/">机器学习</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/about/">关于我</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习学习笔记--线性回归</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-01-19T00:00:00">January 19, 2019</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0" rel="category">机器学习</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		
	</div>
</div>
<div class="content post__content clearfix">
			<p>Andrew NG's Machine Learning.</p>

<h1 id="1-一元线性回归">1. 一元线性回归</h1>

<p>线性回归问题是监督学习中最基础的算法之一。在统计学中，线性回归是利用称为线性回归方程的最小而成函数对一个或多个自变量和因变量之间关系进行建模的一种回归方式。下面就由浅入深的来了解线性回归算法的建模方式。</p>

<h2 id="11-模型表示">1.1 模型表示</h2>

<p>以之前的房屋交易问题为例，假设我们的回归问题的训练集（Traning Set）如下表所示：</p>

<table>
<thead>
<tr>
<th align="center">Size in  $feet^2$  (x)</th>
<th align="center">Price($) in 1000's (y)</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">2104</td>
<td align="center">460</td>
</tr>

<tr>
<td align="center">1416</td>
<td align="center">232</td>
</tr>

<tr>
<td align="center">1534</td>
<td align="center">315</td>
</tr>

<tr>
<td align="center">852</td>
<td align="center">178</td>
</tr>

<tr>
<td align="center">...</td>
<td align="center">...</td>
</tr>
</tbody>
</table>

<p>定义如下几个标记来描述这个回归问题：</p>

<ul>
<li><p>$m$  代表训练集中实例的数量</p></li>

<li><p>$x$  代表特征/输入变量</p></li>

<li><p>$y$  代表目标变量/输出变量</p></li>

<li><p>$\left( x,y \right)$  代表训练集中的实例</p></li>

<li><p>$\left(x^{(i)}, y^{(i)} \right)$ 代表第i个观察实例</p></li>

<li><p>$h$ 代表学习算法的解决方案或函数 也称为假设hypothesis</p></li>
</ul>

<p>于是，监督学习算法的工作方式可以表示为如下图所示：</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7kn2nidsj209f07jweb.jpg" alt=""></figure></p>

<p>在上图中，我们将训练集（房屋价格表）传给学习算法，学习算法的工作就是，假设一个函数，通常用$h$(hypothesis)来表示，该函数的输入是房屋尺寸大小（设为$x$）,$h$会跟根据$x$的值来得出$y$值，$y$值对应的是房屋的价格，</p>

<p>要解决房价预测的问题，我们该如何选择$h$？最简单的表达方式是：</p>

<p><span  class="math">\[
h_\theta \left(x \right) = \theta_{0} + \theta_{1}x
\]</span></p>

<p>因为只含有一个特征/输入变量，因此这种问题被称为<strong>单变量线性回归问题</strong>。</p>

<h2 id="12-代价函数cost-function">1.2 代价函数（Cost Function）</h2>

<p>针对房价预测问题，我们选择了hypothesis函数为：</p>

<p><span  class="math">\[
h_\theta \left(x \right) = \theta_{0} + \theta_{1}x
\]</span></p>

<p>于是，引出了一个问题，我们如何选择$\theta_0$和$\theta<em>1$使得 $h</em>\theta \left(x \right)$能够更加接近真实值$y$。我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中是聚酯之间的差距就是建模误差（modeling error）。如下图所示，蓝线部分就是建模误差。</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7l7ysb9yj207a05rt8n.jpg" alt=""></figure></p>

<p>在线性回归问题中，我们的目标是选择出可以使得建模误差的平方和能够最小的模型参数，即使得代价函数：</p>

<p><span  class="math">\[
J\left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}
\]</span></p>

<p>达到最小。针对课后习题中的样本集，我们来直观的感受一下，$J\left( \theta_0, \theta_1 \right)$在$\theta_0$和$\theta_1$变化的过程中，代价函数得到的值。</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7nlfqrmoj20hs0dcmz3.jpg" alt=""></figure></p>

<p>如果将生成的3D图映射成一维的等高线图，就能更直观的理解代价函数。</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7np7dql5j20hs0dc76c.jpg" alt=""></figure></p>

<p>当然，我们不可能让计算机把这写点的值都算出来，从而找到一个特定的$\theta_0$和$\theta_1$，使得代价函数达到最小。我们所需要的是找到一种有效的算法，能够自动地找出这些使代价函数J取最小值时的参数$\theta_0$和$\theta_1$。</p>

<h2 id="13-梯度下降">1.3 梯度下降</h2>

<p>梯度下降是一个用来求函数最小值的算法，该算法的思路如下：</p>

<ol>
<li><p>随机选取一组$\theta_0$和$\theta_1$</p></li>

<li><p>重复下面的运算，直到收敛</p></li>
</ol>

<p><span  class="math">\[
   \theta_{j} := \theta_{j} - \alpha \frac\partial{\partial\theta_{j}}J\left(\theta_0,\theta_1\right) \qquad \left(for\quad j = 0\quad and\quad  j = 1\right)
   \]</span></p>

<p>其中，$\alpha$为学习率，它决定了我们沿着能让代价函数下降程度最大的方向向下的步长有多大。梯度下降法不能找到全局最小值，而只能找到局部最优解，下图能更直观的理解这句话。</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7ohnr3pgj20dw06w3z5.jpg" alt=""></figure></p>

<p>如上图所示，梯度下降可以理解为，我们站在一座山头，需要尽快的下山，那么我们要做的就是，环顾四周，找到最佳的下山方向，然后迈出一步，接下来重复上面的动作，直到下山。这么一来，我们站在哪个山头将影响着我们下山的最终位置，即我们选择的起始值不同，将影响最终找到的局部最小值。</p>

<p>接下来，我们换一种思路来理解对梯度下降，如下图所示：</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz7osyiq4bj20go091goy.jpg" alt=""></figure></p>

<p>对于$\theta_1$来说，<span  class="math">\(J\left(\theta_1\right)\)</span>是一个二次函数，对$J\left(\theta_1\right)$求导代表求曲线的切线，找到切线方向就是下降最快的方向，图中从红色点开始，一步一步的更新$\theta_1$，最终达到$J\left(\theta_1\right)$的局部最小值。</p>

<p>在梯度下降法中，当我们越接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为局部最低点的倒数会自动变得越来越小。</p>

<p>接下来，我没来推导一下代价函数的求导。</p>

<p><span  class="math">\[
\begin{eqnarray}
\theta_0 & = & \theta_0 - \alpha\frac\partial{\partial\theta_0}\frac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)})-y^{(i)} \right)^{2} \\
& = & \theta_0 - \alpha\frac{1}{m}\sum\limits_{i=1}^m\left(h_{\theta}(x^{(i)})-y^{(i)}\right)
\end{eqnarray}
\]</span></p>

<p><span  class="math">\[
\begin{eqnarray}
\theta_1 & = & \theta_1 - \alpha\frac\partial{\partial\theta_1}\frac{1}{2m}\sum\limits_{i=1}^m \left(h_{\theta}(x^{(i)})-y^{(i)} \right)^{2} \\
& = & \theta_1 - \alpha\frac{1}{m}\sum\limits_{i=1}^m\left(\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x^{(i)} \right)
\end{eqnarray}
\]</span></p>

<p>每次计算$\theta_0$和$theta_1$时，我们都需要对所有样本进行求和运算，因此这个算法也被叫做批量梯度下降算法。在下一节中，我们会应用线性代数的知识来简化这个运算。</p>

<h1 id="2--线性代数基础">2  线性代数基础</h1>

<h2 id="21-矩阵向量">2.1 矩阵向量</h2>

<p>矩阵维数即行数x列数，如$m<em>n$矩阵代表该矩阵有$m$行，$n$列。如下，为一个2</em>3的矩阵：</p>

<p><span  class="math">\[
\left[\begin{matrix}
1 & 2 & 5 \\
3 & 10 & 20
\end{matrix}
\right]
\]</span></p>

<p>矩阵元素即矩阵项$A_{ij}$为第$i$行，第$j$列元素，如$A_{12} = 2$，一般矩阵的元素是从1开始，而不是从0开始。</p>

<h2 id="22-矩阵加减运算">2.2 矩阵加、减运算</h2>

<p>矩阵加法和减法要求两个矩阵的行数和列数必须相同。两个矩阵相加等于矩阵中各个对应位置的矩阵项相加得到的矩阵。即</p>

<p><span  class="math">\[
A \pm B= \left[A_{ij} \pm B_{ij}\right]
\]</span></p>

<p>矩阵加、减运算满足交换律和结合律，如下：</p>

<p><span  class="math">\[
A+B= B+A\\
A+B+C = A+(B+C)
\]</span></p>

<h2 id="23-矩阵标量乘法">2.3 矩阵标量乘法</h2>

<p>数$\lambda$乘以矩阵$A$，等于数$\lambda$乘以矩阵$A$的每一个元素，记为$\lambda A$或者$A\lambda$。特别的，$-A$成为$A$矩阵的负矩阵。</p>

<p>标量乘法满足分配律和结合律。</p>

<p><span  class="math">\[
\lambda \mu A = \lambda (\mu A) \\
\lambda (A+B) = \lambda A  +  \lambda B
\]</span></p>

<h2 id="24-矩阵向量乘法">2.4 矩阵向量乘法</h2>

<p>设存在</p>

<p><span  class="math">\[A = ( a _{ij} ) _{m*s} \\ B=(b _{ij}) _{s*n}\]</span></p>

<p>，则$A$和$B$的乘积$C=AB$是这样一个矩阵：</p>

<ol>
<li>行数与左矩阵即$A$相同，列数与右矩阵即$B$相同，即$C=(C_{ij})_{m*n}$</li>
<li>$C$的第$i$行第$j$列元素等于$A$的第$i$行元素和$B$的第$j$列元素对应相乘，然后再取乘积之和。</li>
</ol>

<p>矩阵乘法运算必须要满足左矩阵的列数和右矩阵的行数相等。</p>

<p><strong>单位矩阵</strong>：$n$阶单位矩阵表示一个$n*n$的矩阵，其主对角线元素均为1，其他元素为0。如2阶单位矩阵：</p>

<p><span  class="math">\[
E_2 = \left[\begin{matrix} 1 & 0\\0 & 1 \end{matrix}\right]
\]</span></p>

<p>对于单位矩阵，满足矩阵乘法交换律$EA=AE$，非单位矩阵不满足乘法交换律。矩阵乘法满足以下定律，即：</p>

<ul>
<li>结合律：$A×B×C=A×(B×C)$</li>

<li><p>分配律：$A×(B \pm C) = A×B \pm A×C$ (左分配律)  $(B \pm C)×A = B×A \pm C×A$ (右分配律)</p></li>

<li><p>$(\lambda A)B = \lambda (AB) = A (\lambda B)$</p></li>
</ul>

<h2 id="25-矩阵转置">2.5 矩阵转置</h2>

<p>将矩阵$A$的行换成同序号的列所得到的新矩阵称为矩阵$A$的转置矩阵，记作$A<sup>{‘}$或$A</sup>{T}$，即：</p>

<p><span  class="math">\[
A _{ij} = \left(A^{T}\right)_{ji}
\]</span></p>

<p>矩阵转置满足一下性质，假设运算都是可行的。</p>

<ul>
<li>$(A ^{T}) ^{T} = A$</li>
<li>$(A+B) ^{T} = A^{T} + B^{T}$</li>
<li>$(AB) ^T = B ^TA ^T$</li>
<li>$(\lambda A)^T = \lambda A^T$， $\lambda$是常数</li>
</ul>

<h2 id="26-逆矩阵">2.6 逆矩阵</h2>

<p>对于矩阵$A$，如果存在一个矩阵$B$，满足$AB = BA =E$， 则称$B$是$A$的逆矩阵，记为$A^{-1}$。逆矩阵满足如下性质：</p>

<ul>
<li>$ (A ^{-1}) ^{-1} = A $</li>
<li>$ (I ^{-1}) ^{-1} = I $</li>
<li>$(M ^{T}) ^{-1} = (M ^{-1}) ^{T}$</li>
<li>$(AB) ^{-1} = B ^{-1}A ^{-1}$</li>
</ul>

<p>求逆矩阵的方法有如下两种，这里不展开介绍，可以参考如下链接<a href="https://blog.csdn.net/zmazon/article/details/8241348">矩阵求逆</a></p>

<ul>
<li>伴随矩阵法</li>
<li>矩阵初等变换法</li>
</ul>

<h1 id="3-多元线性回归">3. 多元线性回归</h1>

<h2 id="31-模型建立">3.1 模型建立</h2>

<p>以上我们讨论了一元线性回归问题，但真实情况是，房价的决定因素并不只有房屋面积，还有房屋楼层，主卧面积等等，于是就构成了一个多线线性回归问题。训练集如下：</p>

<table>
<thead>
<tr>
<th align="center">Size</th>
<th align="center">Number of BedRooms</th>
<th align="center">Number of floors</th>
<th align="center">Age of home(years)</th>
<th align="center">Price</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">2104</td>
<td align="center">5</td>
<td align="center">1</td>
<td align="center">45</td>
<td align="center">460</td>
</tr>

<tr>
<td align="center">1416</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">40</td>
<td align="center">232</td>
</tr>

<tr>
<td align="center">1534</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">40</td>
<td align="center">315</td>
</tr>

<tr>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
</tr>
</tbody>
</table>

<p>这样一来，我们引入一系列新的定义：</p>

<ul>
<li><p>$n$代表特征的数量</p></li>

<li><p>${x^{\left( i \right)}}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）</p></li>

<li><p>${x}_{j}^{\left( i \right)}$代表特征矩阵中第 $i$ 行的第 $j$ 个特征，也就是第 $i$ 个训练实例的第 $j$ 个特征。</p></li>

<li><p>支持多变量的假设函数$h​$表示为：$h_{\theta}\left( x \right)={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}​$，为了使该公式简化，我们假定$x_{0} = 1​$，从而得到$h_{\theta} \left( x \right)={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}​$，用矩阵表示，即$h_{\theta} \left( x \right)={\theta^{T}}X​$</p></li>
</ul>

<h2 id="32-多元梯度下降">3.2 多元梯度下降</h2>

<p>与一元线性回归类似，在多元线性回归中，我们同样用最小二乘法构建了一个代价函数，也就是所有建模误差的平方和，即：
<span  class="math">\(
J\left( {\theta_{0}},{\theta_{1}}...{\theta_{n}} \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}
\)</span>
其中，$h_{\theta}\left( x \right)=\theta^{T}X={\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$</p>

<p>用python来计算代价函数代码如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">computeCostMulti</span>(X, y, theta):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">     Compute cost for linear regression with multiple variables
</span><span style="color:#e6db74">       J = computeCost(X, y, theta) computes the cost of using theta as the
</span><span style="color:#e6db74">       parameter for linear regression to fit the data points in X and y
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    m <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>size
    temp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X , theta)
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sum(np<span style="color:#f92672">.</span>power(temp <span style="color:#f92672">-</span>y, <span style="color:#ae81ff">2</span>)) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> m)</code></pre></div>
<p>这样一来，我们的目标和一元线性回归问题一样，即找出是的代价函数最小的一系列参数。多元线性回归问题的批量梯度下降算法如下：</p>

<p><span  class="math">\[
\theta_j := \theta_j - \alpha \frac\partial{\partial\theta_j} J(\theta_0, \theta_1 ... \theta_n) \\
\]</span></p>

<p>对上式进行求导运算可得：</p>

<p><span  class="math">\[
\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m} \left( \left(h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right) · {x}_j ^{\left( i \right)} \right)
\]</span></p>

<p>那么多元梯度下降的算法就是同步更新$\theta_j$，直到收敛。下面我们来用矩阵运算来简化这个运算，一步得到同步更新后的$\theta_j$值。</p>

<p>首先，我们在特征矩阵$X$的左边添加一行，如下：</p>

<p><span  class="math">\[
X = \left[\begin{matrix} 
1 & x_1^1 & x_1^2 & x_1^3 & x_1^4 \\
1 & x_2^1 & x_2^2 & x_2^3 & x_2^4 \\
1 & x_3^1 & x_3^2 & x_3^3 & x_3^4 \\
... & ...& ...& ...&...
\end{matrix}\right]
\]</span></p>

<p>记 $\theta = \left[\begin{matrix} \theta_0 &amp; \theta_1 &amp; \theta_2 &amp; \theta_3 &amp; \theta_4  \end{matrix}\right]$，</p>

<p><span  class="math">\[
Y = \left[\begin{matrix}y^1 \\ y^2 \\ y^3 \\ y^4 \end{matrix}\right]
\]</span></p>

<p>于是$h_{\theta} \left({x}^{\left( i \right)} \right) - y {^\left(i\right)}$可以记为$X·\theta ^T - Y$，然后，$(X·\theta ^T - Y)^T · X$即可得到矩阵$\theta $ 。这里用Python实现如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradientDescentMulti</span>(X, y, theta, alpha, num_iters):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">     Performs gradient descent to learn theta
</span><span style="color:#e6db74">       theta = gradientDescent(x, y, theta, alpha, num_iters) updates theta by
</span><span style="color:#e6db74">       taking num_iters gradient steps with learning rate alpha
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Initialize some useful values</span>
    J_history <span style="color:#f92672">=</span> []
    m <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>size  <span style="color:#75715e"># number of training examples</span>

    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_iters):
        temp <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(X, theta)
        theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> alpha <span style="color:#f92672">/</span> m <span style="color:#f92672">*</span> (np<span style="color:#f92672">.</span>dot(temp <span style="color:#f92672">-</span> y, X))
        <span style="color:#75715e"># Save the cost J in every iteration</span>
        J_history<span style="color:#f92672">.</span>append(computeCostMulti(X, y, theta))
    
    <span style="color:#66d9ef">return</span> theta, J_history</code></pre></div>
<h2 id="33-特征缩放">3.3 特征缩放</h2>

<p>在多元线性回归问题中，如果我们能保证这些特征都具有相似的尺度，这将使得梯度下降算法更快的收敛。</p>

<p>还是以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0<sub>2000平方英尺，而房间的数量为0</sub>5，以两个参数分别为横纵坐标，绘制代价函数等高线图。如下左图所示：</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz8sqgpn5oj20go09igo7.jpg" alt=""></figure></p>

<p>这样的等高线图表现为更长更扁，在梯度下降的时候，需要更多次的迭代才能使结果收敛；而上右图为特征缩放之后的等高线图，这种情况下可以用更少的迭代使得结果收敛。特征缩放有几种方法如下：</p>

<ol>
<li>调节比例(Rescaling)</li>
</ol>

<p>这种方法是将数据缩放到[0,1]或者之间，缩放到什么范围取决于数据的性质。对于这种方法，其公式如下：</p>

<p><span  class="math">\[
   x^{'} = \frac {x - x_{min}} {x_{max} - x_{min}} \qquad或\qquad x^{'} = \frac {x - x_{mean}} {x_{max} - x_{min}}
   \]</span></p>

<ol start="2">
<li>标准化</li>
</ol>

<p>特征标准化使每个特征有零均值(zero-mean)和单位方差(unit-variance)。这个方法在机器学习中被广泛的使用，例如：SVM、逻辑回归和神经网络。这个方法的公式如下：</p>

<p><span  class="math">\[
   x^{'} =\frac {x - \bar x}{\sigma}
   \]</span></p>

<p>除了特征缩放外，学习率对梯度下降也有一定的影响。</p>

<p>梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，从算法上来看，每次迭代下降的值受到学习率的影响，如果学习率太小，则达到收敛所需要的迭代次数就非常高；如果学习率太大，每次迭代可能不会减小代价函数，可能会越过局部最低点，导致无法收敛。</p>

<p>特征缩放用python代码实现如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np 

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">featureNormalize</span>(X):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">       returns a normalized version of X where
</span><span style="color:#e6db74">       the mean value of each feature is 0 and the standard deviation
</span><span style="color:#e6db74">       is 1. This is often a good preprocessing step to do when
</span><span style="color:#e6db74">       working with learning algorithms.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    mu <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(X, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    sigma <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>std(X, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    X_norm <span style="color:#f92672">=</span> (X <span style="color:#f92672">-</span> mu) <span style="color:#f92672">/</span> sigma

    <span style="color:#66d9ef">return</span> X_norm, mu, sigma</code></pre></div>
<h2 id="34-特征和多项式回归">3.4 特征和多项式回归</h2>

<p>还是以预测房价为例，现在数据集中提供两个特征，分别为$frontage$临街宽度和$depth$纵向深度，这样一来我们构建的$h _\theta(x)$函数如下图所示：</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz9st07ofpj20qo0f077b.jpg" alt=""></figure></p>

<p>但是，这种假设函数特征上存在冗余情况，我们完全可以将临街宽度和纵向深度合成一个特征，即房屋面积。因此对此种数据集来说，我们要预先对特征数据进行合并。</p>

<p><span  class="math">\[
Area \ x = frontage * depth \\
h _\theta(x) = \theta _0 + \theta _1x
\]</span></p>

<p>这样一来，降低了梯度下降算法中的计算量，同时使得模型更加合理。</p>

<p>还有一种情况，线性回归可能并不适用于所有数据，有时我们可能需要用曲线来适应我们的数据集。比如一个二次函数或者一个三次函数。如下图所示：</p>

<p><figure><img src="https://ws1.sinaimg.cn/large/73a1af36ly1fz9u2dca78j20ea07r3yr.jpg" alt=""></figure></p>

<p>这种情况下，我们可以构建一个新的特征，从而将模型转化为线性回归模型。如：</p>

<p><span  class="math">\[
x _1 = x \\
x _2 = x ^ 2 \\
x _3 = x ^ 3 \\
h _\theta (x) = \theta _0 + \theta _1x _1 + \theta _2 x _2 + \theta _3 x_3
\]</span></p>

<p>再比如，如上图中虚线所示，才是符合数据集的模型，这样一来我们可以假设$h _\theta(x)$ 如下：</p>

<p><span  class="math">\[
x _1 = x \\
x _2 = x ^ 2 \\
x _3 = \sqrt x \\
h _\theta (x) = \theta _0 + \theta _1x _1 + \theta _2 x _2 + \theta _3 x_3
\]</span></p>

<blockquote>
<p>Important: 如果使用多项式回归，在进行建模前，特征缩放显得非常重要，且必不可少</p>
</blockquote>

<p>因此，在拿到数据集的时候，我们需要先观察数据集适合哪种模型，然后通过合并特征以及在已有特征的基础上新建多项式特征使得模型更加合理，然后在进行建模运算。</p>

<h2 id="35-正规方程">3.5 正规方程</h2>

<p>在前面的小节中，我们都是用梯度下降算法来求代价函数$J(\theta)$的局部最小值，但是对于某些线性回归问题，正规方程可能是更好的解决方法，我们先来推到一下正规方程如何求代价函数的最小值。</p>

<p><span  class="math">\[
J(\theta) =\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( h_{\theta} \left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}} = (H _\theta(X) - Y)^T(H _\theta(X) - Y)
\]</span></p>

<p>其中，参数矩阵为$\theta=(\theta _0 \ \theta _1 \ \theta _2 \ ... \theta _m) ^T$ , $h _\theta(X) = X \theta$, $Y = (y _1\ y _2 \ ...\ y _n)^T$。当$\frac {d}{d _\theta }J(\theta) = 0$时，此时代价函数达到最小值，满足此条件的$\theta$即为所求。</p>

<p>在代价函数求导之前，我们先复习一下矩阵求导的基础知识。</p>

<ul>
<li>矩阵的迹</li>
</ul>

<p><span  class="math">\[
tr A = \sum\limits_{i=1}^{n} A _{ii}  
\]</span></p>

<ul>
<li>矩阵的迹简单运算</li>
</ul>

<p><span  class="math">\[
\begin{eqnarray}
a &=& tr \ a \qquad a \in \mathbb{R} \\
tr AB &=& tr BA \\
tr ABC &=& trCBA
\end{eqnarray}
\]</span></p>

<ul>
<li>迹求导运算</li>
</ul>

<p><span  class="math">\[
\begin{eqnarray}
\nabla _A tr AB &=& \nabla _A tr BA = B^T \\
\nabla _{A^T}a &=& (\nabla _A a)^T  \\
\nabla _A tr ABA^TC &=& CAB + C^TAB^T
\end{eqnarray}
\]</span></p>

<p>接下来，开始推导。</p>

<p><span  class="math">\[
\begin{eqnarray}
\nabla _\theta J(\theta) &=& \nabla _\theta(X\theta -Y)^T(X\theta -Y) \\
&=& \nabla _\theta(\theta^TX^T - Y^T)(X\theta -Y) \\
&=& \nabla _\theta(\theta^TX^TX\theta - Y^TX\theta - \theta^TX^TY - Y^TY) \tag{1}\\
&=& \nabla _\theta(\theta^TX^TX\theta - 2\theta^TX^TY) \\
&=& \nabla _\theta tr (\theta^TX^TX\theta - 2\theta^TX^TY) \\ 
&=& \nabla _\theta tr (\theta\theta^TX^TX) - 2 Y^TX \tag{2} \\ 
&=& \nabla _\theta tr (\theta I \theta^TX^TX) - 2 Y^TX \\
&=& 2X^T X\theta - 2 Y^TX \\
\end{eqnarray}
\]</span></p>

<p>其中公式$(1)$处，$\theta ^T X ^T Y$是标量，因为$\theta ^ T$是$1 * m$矩阵，$X ^ T$是$m * n$矩阵，$Y$是$n * 1$矩阵，该式结果为$1 * 1$矩阵，即标量。</p>

<p>公式$(2)$处，$\theta\theta ^T X ^T X = \theta I \theta ^T X ^ TX$符合迹求导公式$\nabla _A tr ABA ^T C = CAB + C ^T AB ^T$。</p>

<p>最后我们令$2X ^T X \theta - 2 Y ^T X = 0$, 即得$\theta = (X ^T X) ^ {-1} Y ^T X$，推导完毕。</p>

<p>梯度下降和正规方程求$\theta$的优劣势对比如下：</p>

<table>
<thead>
<tr>
<th align="center">梯度下降</th>
<th align="center">正规方程</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">需要选择学习率</td>
<td align="center">不需要选择学习率</td>
</tr>

<tr>
<td align="center">需要大量迭代</td>
<td align="center">不需要迭代</td>
</tr>

<tr>
<td align="center">复杂率$O(kn^2)$</td>
<td align="center">复杂率$O(n^3)，需要计算逆矩阵$</td>
</tr>

<tr>
<td align="center">大量数据集时也能表现很好</td>
<td align="center">大量数据集时运行很慢</td>
</tr>
</tbody>
</table>

<p>在计算正规方程时，$X ^T X$ 不一定可逆，主要原因如下：</p>

<ul>
<li>冗余特征，其中两个特征密切相关（即他们是线性相关的）。</li>
<li>特征太多(如m&lt;=n), 在这种情况下，删除一些特征或者使用特征化。</li>
</ul>

<p>因此，在进行正规方程计算时，需要删除一些冗余特征或者当特征太多时，删除一些特征。这点非常重要。</p>

<p>正规方程的python实现如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np  
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalEqn</span>(X, y):
    <span style="color:#e6db74">&#34;&#34;&#34; Computes the closed-form solution to linear regression
</span><span style="color:#e6db74">       normalEqn(X,y) computes the closed-form solution to linear
</span><span style="color:#e6db74">       regression using the normal equations.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    temp <span style="color:#f92672">=</span>  np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>inv(np<span style="color:#f92672">.</span>dot(X<span style="color:#f92672">.</span>T, X))
    temp1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(temp, X<span style="color:#f92672">.</span>T)
    theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>dot(temp1, y<span style="color:#f92672">.</span>T)
    <span style="color:#66d9ef">return</span> theta</code></pre></div>
<blockquote>
<p>附赠：</p>

<p>课后习题python代码答案版：<a href="https://github.com/Zeeech/Machine-Learning/tree/master/ex1">coursera-ML-python</a></p>
</blockquote>

<h1 id="参考">参考</h1>

<p><a href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术（上）</a></p>

<p><a href="https://blog.csdn.net/chenlin41204050/article/details/78220280">正规方程求解特征参数的推导过程</a></p>
		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/machinelearning/" rel="tag">MachineLearning</a></li>
	</ul>
</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Zeech avatar" src="/img/avatar.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Zeech</span>
	</div>
	<div class="authorbox__description">
		Nothing to say
	</div>
</div>

<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/machinelearning/machinelearningintro/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">机器学习学习笔记--简介</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/machinelearning/machinelearninglogisticregression/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">机器学习学习笔记--逻辑回归</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "Zeecoder" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="https://zcheng.ren" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/machinelearning/machinelearninglogisticregression/">机器学习学习笔记--逻辑回归</a></li>
			<li class="widget__item"><a class="widget__link" href="/machinelearning/machinelearninglinearregression/">机器学习学习笔记--线性回归</a></li>
			<li class="widget__item"><a class="widget__link" href="/machinelearning/machinelearningintro/">机器学习学习笔记--简介</a></li>
			<li class="widget__item"><a class="widget__link" href="/2018/08/09/pythondatetimezone/">Python时区问题</a></li>
			<li class="widget__item"><a class="widget__link" href="/2018/07/30/learncocoapods/">三分钟学会cocoapods</a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/categories/redis">Redis</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e5%bc%80%e5%8f%91%e5%b7%a5%e5%85%b7">开发工具</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0">机器学习</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e6%ba%90%e7%a0%81%e9%98%85%e8%af%bb">源码阅读</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e7%94%9f%e6%b4%bb%e6%80%9d%e8%80%83">生活思考</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e7%bc%96%e7%a8%8b%e6%8a%80%e6%9c%af">编程技术</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/%e8%af%bb%e4%b9%a6%e7%ac%94%e8%ae%b0">读书笔记</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/blog" title="Blog">Blog</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/c&#43;&#43;" title="C&#43;&#43;">C&#43;&#43;</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/c&#43;&#43;%e7%9f%a5%e8%af%86" title="C&#43;&#43;知识">C&#43;&#43;知识</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/cocoapads" title="Cocoapads">Cocoapads</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/go" title="Go">Go</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/ios" title="Ios">Ios</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/linux" title="Linux">Linux</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mac" title="Mac">Mac</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/machinelearning" title="Machinelearning">Machinelearning</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/nginx" title="Nginx">Nginx</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/python" title="Python">Python</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/redis" title="Redis">Redis</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/sourcecode" title="Sourcecode">Sourcecode</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/spacevim" title="Spacevim">Spacevim</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/stl" title="Stl">Stl</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/themes" title="Themes">Themes</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/unix%e7%bd%91%e7%bb%9c%e7%bc%96%e7%a8%8b" title="Unix网络编程">Unix网络编程</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/vim" title="Vim">Vim</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/%e5%b9%b4%e5%ba%a6%e6%80%bb%e7%bb%93" title="年度总结">年度总结</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/%e6%95%b0%e6%8d%ae%e7%bb%93%e6%9e%84" title="数据结构">数据结构</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/%e7%ae%97%e6%b3%95" title="算法">算法</a>
	</div>
</div>
<div class="widget-social widget">
	<h4 class="widget-social__title widget__title">Social</h4>
	<div class="widget-social__content widget__content">
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Facebook" rel="noopener noreferrer" href="https://facebook.com/zhang.cheng.50746" target="_blank">
				<svg class="widget-social__link-icon icon-facebook" viewBox="0 0 352 352" width="24" height="24" fill="#fff"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5 0 32-14.5 32-32v-288c0-17.5-14.5-32-32-32h-288c-17.5 0-32 14.5-32 32zm320 0v288h-83v-108h41.5l6-48h-47.5v-31c0-14 3.5-23.5 23.5-23.5h26v-43.5c-4.4-.6-19.8-1.5-37.5-1.5-36.9 0-62 22.2-62 63.5v36h-42v48h42v108h-155v-288z"/></svg>
				<span>Facebook</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Twitter" rel="noopener noreferrer" href="https://twitter.com/Zeeech" target="_blank">
				<svg class="widget-social__link-icon icon-twitter" viewBox="0 0 384 312" width="24" height="24" fill="#fff"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
				<span>Twitter</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Telegram" rel="noopener noreferrer" href="https://t.me/Zeeeech" target="_blank">
				<svg class="widget-social__link-icon icon-telegram" viewBox="0 0 132 110" width="24" height="24"><path fill="#ddd" d="M50 103c-4 0-3-1-5-5L34 60l88-52"/><path fill="#aaa" d="M50 103c3 0 4-1 6-3l16-16-20-12"/><path fill="#fff" d="M52 72l48 36c6 3 10 2 11-5l20-93c2-8-3-11-8-9L7 45c-8 4-8 8-1 10l29 9 69-43c3-2 6-1 4 1"/></svg>
				<span>Telegram</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="GitHub" rel="noopener noreferrer" href="https://github.com/Zeecoders" target="_blank">
				<svg class="widget-social__link-icon icon-github" viewBox="0 0 384 374" width="24" height="24" fill="#fff"><path d="m192 0c-106.1 0-192 85.8-192 191.7 0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2 0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8 0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7 0 0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4 0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5 0 25.6-.2 46.3-.2 52.6 0 5.1 3.5 11.1 13.2 9.2 76.2-25.5 131.2-97.3 131.2-182 0-105.9-86-191.7-192-191.7z"/></svg>
				<span>GitHub</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Email" href="mailto:zcheng1004@163.com">
				<svg class="widget-social__link-icon icon-mail" viewBox="0 0 416 288" width="24" height="24" fill="#fff"><path d="m0 16v256 16h16 384 16v-16-256-16h-16-384-16zm347 16-139 92.5-139-92.5zm-148 125.5 9 5.5 9-5.5 167-111.5v210h-352v-210z"/></svg>
				<span>zcheng1004@163.com</span>
			</a>
		</div>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 @zeech.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          processEscapes: true,
          processEnvironments: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          TeX: { equationNumbers: { autoNumber: "AMS" },
               extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
      });
    </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>

</body>
</html>